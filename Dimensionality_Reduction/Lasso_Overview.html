<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Learn about Lasso, a dimensionality reduction method. Comprehensive overview with theory, applications, and use cases for machine learning and data science.">
  <link rel="canonical" href="https://yourdomain.com/Dimensionality_Reduction/Lasso_Overview.html">
  <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "name": "Lasso - Overview & Theory | Dimensionality Reduction | ML Tools",
  "description": "Learn about Lasso, a dimensionality reduction method. Comprehensive overview with theory, applications, and use cases for machine learning and data science.",
  "url": "https://yourdomain.com/Dimensionality_Reduction/Lasso_Overview.html",
  "about": {
    "@type": "Thing",
    "name": "Lasso",
    "description": "Lasso algorithm for dimensionality reduction"
  },
  "headline": "Lasso",
  "articleSection": "Dimensionality Reduction"
}
  </script>
    <title>Lasso - Overview & Theory | Dimensionality Reduction | ML Tools</title>
    <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@200;300;400;600;700;800;900&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Nunito', sans-serif;
            color: #cfcfcf;
            background-color: #1e1e1e;
            margin: 0;
            padding: 0;
            display: flex;
            min-height: 100vh;
        }
        .sidebar {
            width: 250px;
            background-color: #171717;
            padding: 30px 20px;
            border-right: 1px solid #333;
            overflow-y: auto;
            height: 100vh;
        }
        .sidebar h1 { color: white; font-size: 1.8rem; font-weight: bold; margin-bottom: 30px; text-align: center; }
        .nav-section { margin-bottom: 30px; }
        .nav-section h3 { color: #dd8448; font-size: 0.9rem; font-weight: 600; text-transform: uppercase; margin-bottom: 15px; letter-spacing: 1px; }
        .nav-item { display: block; color: #676767; text-decoration: none; padding: 12px 15px; margin-bottom: 5px; border-radius: 8px; transition: all 0.3s ease; font-weight: 500; }
        .nav-item:hover { background-color: #2d2d2d; color: white; transform: translateX(5px); }
        .nav-item.active { background-color: #dd8448; color: white; }
        .nav-category { margin-bottom: 20px; }
        .nav-category-header { color: #dd8448; font-size: 0.85rem; font-weight: 600; text-transform: uppercase; padding: 10px 15px; cursor: pointer; border-radius: 8px; transition: background-color 0.3s ease; display: flex; justify-content: space-between; align-items: center; letter-spacing: 1px; }
        .nav-category-header:hover { background-color: #2d2d2d; }
        .nav-category-header .arrow { transition: transform 0.3s ease; font-size: 0.7rem; }
        .nav-category-header.collapsed .arrow { transform: rotate(-90deg); }
        .nav-subcategory { margin-left: 15px; margin-top: 10px; overflow: hidden; max-height: 1200px; transition: max-height 0.3s ease; }
        .nav-subcategory.collapsed { max-height: 0; }
        .nav-subheader { color: #999; font-size: 0.75rem; font-weight: 600; text-transform: uppercase; padding: 8px 15px; margin-top: 5px; letter-spacing: 0.5px; cursor: pointer; display: flex; justify-content: space-between; align-items: center; border-radius: 6px; transition: background-color 0.3s ease; }
        .nav-subheader:hover { background-color: #2d2d2d; }
        .nav-subheader .arrow { font-size: 0.6rem; transition: transform 0.3s ease; }
        .nav-subheader.collapsed .arrow { transform: rotate(-90deg); }
        .nav-subheader.category-header { color: #dd8448; font-size: 0.85rem; font-weight: 700; padding: 10px 15px; margin-top: 8px; letter-spacing: 1px; border: 1px solid rgba(221,132,72,0.3); }
        .nav-subheader.category-header:hover { background-color: rgba(221,132,72,0.15); border-color: rgba(221,132,72,0.5); }
        .nav-subgroup { overflow: hidden; max-height: 200px; transition: max-height 0.3s ease; padding-bottom: 5px; }
        .nav-subgroup.collapsed { max-height: 0; }
        .nav-subgroup.category-group { max-height: 1000px; }
        .nav-subgroup.category-group.collapsed { max-height: 0; }
        .nav-subheader.nested { margin-left: 10px; font-size: 0.7rem; }
        .nav-subgroup.nested { margin-left: 10px; max-height: 200px; }
        .nav-subheader.subcategory-header { color: #999; font-size: 0.75rem; font-weight: 600; padding: 8px 15px; margin-left: 10px; margin-top: 5px; letter-spacing: 0.5px; border: 1px solid rgba(153, 153, 153, 0.2); }
        .nav-subheader.subcategory-header:hover { background-color: rgba(45, 45, 45, 0.5); border-color: rgba(153, 153, 153, 0.4); }
        .nav-subgroup.subcategory-group { margin-left: 10px; max-height: 2000px; overflow: hidden; transition: max-height 0.3s ease; padding-bottom: 5px; }
        .nav-subgroup.subcategory-group.collapsed { max-height: 0; }
        .nav-item.sub { padding: 8px 15px; font-size: 0.9rem; margin-left: 10px; margin-bottom: 5px; }
        .main-content {
            flex: 1;
            padding: 40px;
        }
        .title {
            color: #ffffff;
            font-size: 2.2rem;
            font-weight: 800;
            margin: 0 0 8px 0;
        }
        .subtitle {
            color: #8a8a8a;
            font-size: 1rem;
            margin-bottom: 28px;
        }
        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
            gap: 20px;
            margin-bottom: 28px;
        }
        .card {
            background-color: #171717;
            border: 1px solid #333;
            border-radius: 12px;
            padding: 22px;
            transition: border-color 0.3s ease, transform 0.3s ease;
        }
        .card:hover { border-color: #dd8448; transform: translateY(-3px); }
        .card h3 { color: #ffffff; margin: 0 0 8px 0; font-size: 1.15rem; }
        .card p { color: #9a9a9a; margin: 0; line-height: 1.6; }
        .section {
            background-color: #171717;
            border: 1px solid #333;
            border-radius: 12px;
            padding: 24px;
            margin-bottom: 24px;
        }
        .section h2 {
            color: #ffffff;
            font-size: 1.5rem;
            margin: 0 0 12px 0;
        }
        .badge {
            display: inline-block;
            background: rgba(221,132,72,0.15);
            color: #ffb07a;
            border: 1px solid rgba(221,132,72,0.35);
            padding: 2px 8px;
            border-radius: 999px;
            font-size: 12px;
            margin-left: 8px;
        }
        .list { margin: 0; padding-left: 18px; color: #b0b0b0; }
        .list li { margin-bottom: 6px; }
        .split {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 16px;
        }
        .callout {
            background: #202020;
            border: 1px dashed #3a3a3a;
            border-radius: 10px;
            padding: 14px;
            color: #a9a9a9;
        }
        .kpi {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(160px, 1fr));
            gap: 10px;
        }
        .kpi .item {
            background: #1c1c1c;
            border: 1px solid #2a2a2a;
            border-radius: 10px;
            padding: 10px 12px;
        }
        .kpi .label { color: #8e8e8e; font-size: 12px; }
        .kpi .value { color: #ffffff; font-weight: 700; font-size: 18px; }
        .muted { color: #9a9a9a; }
        .accent { color: #dd8448; }
    </style>
</head>
<body>
    <div class="sidebar"></div>
    <div class="main-content">
        <h1 class="title">Lasso (L1 Regularization) Overview <span class="badge">Guide</span></h1>
        <div class="subtitle">An embedded method that performs feature selection during model training using L1 regularization — coefficients are driven to zero, effectively removing irrelevant features.</div>

        <div class="grid">
            <div class="card">
                <h3>Primary Uses</h3>
                <p>Feature selection during regression, handling multicollinearity, sparse model creation, automatic feature elimination, regularization to prevent overfitting, interpretable models with fewer features.</p>
            </div>
            <div class="card">
                <h3>Strengths</h3>
                <p>Automatic feature selection (no separate step), handles multicollinearity, produces sparse models, prevents overfitting, computationally efficient, works well with high-dimensional data, provides regularization path, interpretable results.</p>
            </div>
            <div class="card">
                <h3>Watch-outs</h3>
                <p>Requires target variable (supervised only), sensitive to alpha parameter, may select only one feature from correlated groups, requires standardization, may underfit with large alpha, limited to linear relationships, requires careful hyperparameter tuning.</p>
            </div>
        </div>

        <div class="section">
            <h2>How It Works <span class="badge">Mathematical Foundation</span></h2>
            <div class="split">
                <div>
                    <p class="muted">Lasso (Least Absolute Shrinkage and Selection Operator) is an <span class="accent">embedded method</span> that adds L1 regularization to linear regression. The L1 penalty drives some coefficients to exactly zero, effectively performing feature selection during training.</p>
                    <ul class="list">
                        <li><strong>Objective Function</strong>: Minimize (1/2n)||y - Xw||² + α||w||₁, where ||w||₁ = Σ|w_i| is the L1 norm (sum of absolute coefficients).</li>
                        <li><strong>L1 Regularization</strong>: The α||w||₁ term penalizes large coefficients and drives some to exactly zero, unlike L2 (Ridge) which only shrinks them.</li>
                        <li><strong>Feature Selection</strong>: Features with zero coefficients are effectively removed from the model, providing automatic feature selection.</li>
                        <li><strong>Coordinate Descent</strong>: Lasso is typically solved using coordinate descent, updating one coefficient at a time while keeping others fixed.</li>
                        <li><strong>Regularization Path</strong>: As α increases, more coefficients become zero, creating a regularization path from all features to few features.</li>
                        <li><strong>Sparsity</strong>: Lasso promotes sparsity (many zero coefficients), making models more interpretable and reducing overfitting.</li>
                    </ul>
                </div>
                <div class="callout">Tip: Lasso is particularly useful when you have many features and want automatic feature selection. The α parameter controls the strength of regularization: larger α means more features are removed. Use cross-validation to find the optimal α value.</div>
            </div>
        </div>

        <div class="section">
            <h2>Feature Selection Statistics <span class="badge">Interpretation</span></h2>
            <div class="kpi">
                <div class="item"><div class="label">Original Features</div><div class="value">Number of input features</div></div>
                <div class="item"><div class="label">Selected Features</div><div class="value">Features with non-zero coefficients</div></div>
                <div class="item"><div class="label">Removed Features</div><div class="value">Features with zero coefficients</div></div>
                <div class="item"><div class="label">Alpha (α)</div><div class="value">Regularization strength parameter</div></div>
                <div class="item"><div class="label">Sparsity Ratio</div><div class="value">Proportion of zero coefficients</div></div>
                <div class="item"><div class="label">R² Score</div><div class="value">Model performance metric</div></div>
            </div>
        </div>

        <div class="section">
            <h2>Reading the Visualizations <span class="badge">Analysis</span></h2>
            <ul class="list">
                <li><strong>Coefficient Path</strong>: Shows how coefficients change as α increases. Features whose paths cross zero are eliminated at that α value.</li>
                <li><strong>Coefficient Magnitude</strong>: Bar chart showing final coefficient values. Zero coefficients indicate removed features.</li>
                <li><strong>Regularization Path</strong>: Line plot showing number of selected features vs α. Helps identify optimal α for desired sparsity.</li>
                <li><strong>Performance vs Alpha</strong>: Shows model performance (R²) across different α values. Helps find optimal regularization strength.</li>
                <li><strong>Feature Importance</strong>: Bar chart of coefficient magnitudes, sorted by importance. Helps identify most important features.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Parameter Guidance <span class="badge">Best Practice</span></h2>
            <ul class="list">
                <li><strong>Alpha (α)</strong>: Regularization strength. Common range: 0.001 to 1.0. Use cross-validation to find optimal value. Larger α = more features removed.</li>
                <li><strong>Max Iterations</strong>: Maximum coordinate descent iterations. Common values: 1000-10000. Increase if convergence is slow.</li>
                <li><strong>Tolerance</strong>: Convergence threshold. Common values: 1e-4 to 1e-6. Smaller = more accurate but slower.</li>
                <li><strong>Standardize Features</strong>: Always standardize features before Lasso. This ensures fair coefficient comparison and proper regularization.</li>
                <li><strong>Warm Start</strong>: Use previous solution as initialization for next α. Speeds up regularization path computation.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Data Preparation <span class="badge">Best Practice</span></h2>
            <ul class="list">
                <li><strong>Handle Missing Values</strong>: Remove or impute missing values before Lasso. Lasso requires complete data.</li>
                <li><strong>Standardize Features</strong>: Always standardize features (mean=0, std=1) before applying Lasso. This is critical for proper regularization.</li>
                <li><strong>Target Variable</strong>: Lasso requires a target variable (supervised learning). Ensure target is numeric for regression.</li>
                <li><strong>Sample Size</strong>: Lasso works well with moderate to large sample sizes (n > 50). For small datasets, use smaller α values.</li>
                <li><strong>Feature Scaling</strong>: Standardization is essential. Without it, Lasso will unfairly penalize features with larger scales.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Common Pitfalls <span class="badge">Avoid</span></h2>
            <ul class="list">
                <li><strong>Not Standardizing</strong>: Failing to standardize features will bias Lasso toward features with larger scales. Always standardize first.</li>
                <li><strong>Alpha Too Large</strong>: Using too large α will remove too many features, leading to underfitting. Use cross-validation to find optimal α.</li>
                <li><strong>Alpha Too Small</strong>: Using too small α will keep too many features, reducing sparsity benefits. Balance between performance and sparsity.</li>
                <li><strong>Ignoring Multicollinearity</strong>: Lasso may arbitrarily select one feature from a correlated group. Be aware of this behavior.</li>
                <li><strong>Not Tuning Alpha</strong>: Using default α without tuning will likely give suboptimal results. Always tune α with cross-validation.</li>
                <li><strong>Overfitting</strong>: Even with regularization, Lasso can overfit if α is too small. Monitor validation performance.</li>
                <li><strong>Linear Assumption</strong>: Lasso assumes linear relationships. For non-linear relationships, consider polynomial features or other methods.</li>
            </ul>
        </div>

        <div class="section">
            <h2>When to Use <span class="badge">Application</span></h2>
            <ul class="list">
                <li><strong>High-Dimensional Data</strong>: When you have many features (p > n or p ≈ n) and want automatic feature selection.</li>
                <li><strong>Sparse Models</strong>: When you want a sparse model with only the most important features for interpretability.</li>
                <li><strong>Multicollinearity</strong>: When features are highly correlated and you want Lasso to select representative features.</li>
                <li><strong>Automatic Selection</strong>: When you want feature selection to happen automatically during training, not as a separate step.</li>
                <li><strong>Regularization</strong>: When you want to prevent overfitting while also performing feature selection.</li>
                <li><strong>Interpretable Models</strong>: When you need an interpretable model with fewer features for easier understanding.</li>
                <li><strong>Regression Tasks</strong>: When performing regression with continuous target variables.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Alternatives to Consider <span class="badge">Comparison</span></h2>
            <ul class="list">
                <li><strong>vs Ridge Regression (L2)</strong>: Ridge shrinks coefficients but doesn't set them to zero. Lasso provides feature selection, Ridge doesn't. Use Lasso for selection, Ridge for shrinkage.</li>
                <li><strong>vs Elastic Net</strong>: Elastic Net combines L1 and L2 penalties. More stable than Lasso with correlated features. Use Elastic Net when features are highly correlated.</li>
                <li><strong>vs Wrapper Methods</strong>: Wrapper methods use cross-validation, Lasso uses regularization. Wrapper methods are slower but may be more accurate. Use Lasso for speed, wrappers for accuracy.</li>
                <li><strong>vs Filter Methods</strong>: Filter methods are fast but model-agnostic. Lasso is model-aware but slower. Use filters for quick ranking, Lasso for final selection.</li>
                <li><strong>vs RFE</strong>: RFE removes features iteratively, Lasso does it during training. RFE is more flexible, Lasso is faster. Use RFE for flexibility, Lasso for speed.</li>
                <li><strong>vs Forward/Backward Selection</strong>: Forward/Backward use CV, Lasso uses regularization. Forward/Backward are slower but may find better subsets. Use Lasso for speed, Forward/Backward for thoroughness.</li>
            </ul>
        </div>
    </div>
    <script src="../sidebar.js"></script>
</body>
</html>

