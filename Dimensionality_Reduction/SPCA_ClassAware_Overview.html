<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Learn about SPCA (Class-Aware), a dimensionality reduction method. Comprehensive overview with theory, applications, and use cases for machine learning and data science.">
  <link rel="canonical" href="https://yourdomain.com/Dimensionality_Reduction/SPCA_ClassAware_Overview.html">
  <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "name": "SPCA (Class-Aware) - Overview & Theory | Dimensionality Reduction | ML Tools",
  "description": "Learn about SPCA (Class-Aware), a dimensionality reduction method. Comprehensive overview with theory, applications, and use cases for machine learning and data science.",
  "url": "https://yourdomain.com/Dimensionality_Reduction/SPCA_ClassAware_Overview.html",
  "about": {
    "@type": "Thing",
    "name": "SPCA (Class-Aware)",
    "description": "SPCA (Class-Aware) algorithm for dimensionality reduction"
  },
  "headline": "SPCA (Class-Aware)",
  "articleSection": "Dimensionality Reduction"
}
  </script>
    <title>SPCA (Class-Aware) - Overview & Theory | Dimensionality Reduction | ML Tools</title>
    <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@200;300;400;600;700;800;900&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Nunito', sans-serif;
            color: #cfcfcf;
            background-color: #1e1e1e;
            margin: 0;
            padding: 0;
            display: flex;
            min-height: 100vh;
        }
        .sidebar {
            width: 250px;
            background-color: #171717;
            padding: 30px 20px;
            border-right: 1px solid #333;
            overflow-y: auto;
            height: 100vh;
        }
        .sidebar h1 { color: white; font-size: 1.8rem; font-weight: bold; margin-bottom: 30px; text-align: center; }
        .nav-section { margin-bottom: 30px; }
        .nav-section h3 { color: #dd8448; font-size: 0.9rem; font-weight: 600; text-transform: uppercase; margin-bottom: 15px; letter-spacing: 1px; }
        .nav-item { display: block; color: #676767; text-decoration: none; padding: 12px 15px; margin-bottom: 5px; border-radius: 8px; transition: all 0.3s ease; font-weight: 500; }
        .nav-item:hover { background-color: #2d2d2d; color: white; transform: translateX(5px); }
        .nav-item.active { background-color: #dd8448; color: white; }
        .nav-category { margin-bottom: 20px; }
        .nav-category-header { color: #dd8448; font-size: 0.85rem; font-weight: 600; text-transform: uppercase; padding: 10px 15px; cursor: pointer; border-radius: 8px; transition: background-color 0.3s ease; display: flex; justify-content: space-between; align-items: center; letter-spacing: 1px; }
        .nav-category-header:hover { background-color: #2d2d2d; }
        .nav-category-header .arrow { transition: transform 0.3s ease; font-size: 0.7rem; }
        .nav-category-header.collapsed .arrow { transform: rotate(-90deg); }
        .nav-subcategory { margin-left: 15px; margin-top: 10px; overflow: hidden; max-height: 1200px; transition: max-height 0.3s ease; }
        .nav-subcategory.collapsed { max-height: 0; }
        .nav-subheader { color: #999; font-size: 0.75rem; font-weight: 600; text-transform: uppercase; padding: 8px 15px; margin-top: 5px; letter-spacing: 0.5px; cursor: pointer; display: flex; justify-content: space-between; align-items: center; border-radius: 6px; transition: background-color 0.3s ease; }
        .nav-subheader:hover { background-color: #2d2d2d; }
        .nav-subheader .arrow { font-size: 0.6rem; transition: transform 0.3s ease; }
        .nav-subheader.collapsed .arrow { transform: rotate(-90deg); }
        .nav-subheader.category-header { color: #dd8448; font-size: 0.85rem; font-weight: 700; padding: 10px 15px; margin-top: 8px; letter-spacing: 1px; border: 1px solid rgba(221,132,72,0.3); }
        .nav-subheader.category-header:hover { background-color: rgba(221,132,72,0.15); border-color: rgba(221,132,72,0.5); }
        .nav-subgroup { overflow: hidden; max-height: 200px; transition: max-height 0.3s ease; padding-bottom: 5px; }
        .nav-subgroup.collapsed { max-height: 0; }
        .nav-subgroup.category-group { max-height: 1000px; }
        .nav-subgroup.category-group.collapsed { max-height: 0; }
        .nav-subheader.nested { margin-left: 10px; font-size: 0.7rem; }
        .nav-subgroup.nested { margin-left: 10px; max-height: 200px; }
        .nav-subheader.subcategory-header { color: #999; font-size: 0.75rem; font-weight: 600; padding: 8px 15px; margin-left: 10px; margin-top: 5px; letter-spacing: 0.5px; border: 1px solid rgba(153, 153, 153, 0.2); }
        .nav-subheader.subcategory-header:hover { background-color: rgba(45, 45, 45, 0.5); border-color: rgba(153, 153, 153, 0.4); }
        .nav-subgroup.subcategory-group { margin-left: 10px; max-height: 2000px; overflow: hidden; transition: max-height 0.3s ease; padding-bottom: 5px; }
        .nav-subgroup.subcategory-group.collapsed { max-height: 0; }
        .nav-item.sub { padding: 8px 15px; font-size: 0.9rem; margin-left: 10px; margin-bottom: 5px; }
        .main-content {
            flex: 1;
            padding: 40px;
        }
        .title {
            color: #ffffff;
            font-size: 2.2rem;
            font-weight: 800;
            margin: 0 0 8px 0;
        }
        .subtitle {
            color: #8a8a8a;
            font-size: 1rem;
            margin-bottom: 28px;
        }
        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
            gap: 20px;
            margin-bottom: 28px;
        }
        .card {
            background-color: #171717;
            border: 1px solid #333;
            border-radius: 12px;
            padding: 22px;
            transition: border-color 0.3s ease, transform 0.3s ease;
        }
        .card:hover { border-color: #dd8448; transform: translateY(-3px); }
        .card h3 { color: #ffffff; margin: 0 0 8px 0; font-size: 1.15rem; }
        .card p { color: #9a9a9a; margin: 0; line-height: 1.6; }
        .section {
            background-color: #171717;
            border: 1px solid #333;
            border-radius: 12px;
            padding: 24px;
            margin-bottom: 24px;
        }
        .section h2 {
            color: #ffffff;
            font-size: 1.5rem;
            margin: 0 0 12px 0;
        }
        .badge {
            display: inline-block;
            background: rgba(221,132,72,0.15);
            color: #ffb07a;
            border: 1px solid rgba(221,132,72,0.35);
            padding: 2px 8px;
            border-radius: 999px;
            font-size: 12px;
            margin-left: 8px;
        }
        .list { margin: 0; padding-left: 18px; color: #b0b0b0; }
        .list li { margin-bottom: 6px; }
        .split {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 16px;
        }
        .callout {
            background: #202020;
            border: 1px dashed #3a3a3a;
            border-radius: 10px;
            padding: 14px;
            color: #a9a9a9;
        }
        .kpi {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(160px, 1fr));
            gap: 10px;
        }
        .kpi .item {
            background: #1c1c1c;
            border: 1px solid #2a2a2a;
            border-radius: 10px;
            padding: 10px 12px;
        }
        .kpi .label { color: #8e8e8e; font-size: 12px; }
        .kpi .value { color: #ffffff; font-weight: 700; font-size: 18px; }
        .muted { color: #9a9a9a; }
        .accent { color: #dd8448; }
    </style>
</head>
<body>
    <div class="sidebar"></div>
    <div class="main-content">
        <h1 class="title">SPCA (Class-Aware) Overview <span class="badge">Guide</span></h1>
        <div class="subtitle">Supervised Principal Component Analysis for Class-Aware Methods: supervised dimensionality reduction that uses class labels to guide PCA, maximizing variance while considering class structure.</div>

        <div class="grid">
            <div class="card">
                <h3>Primary Uses</h3>
                <p>Supervised dimensionality reduction, feature extraction for classification, class-aware visualization, reducing dimensions while preserving class structure, preprocessing for classification algorithms, class-guided variance maximization.</p>
            </div>
            <div class="card">
                <h3>Strengths</h3>
                <p>Uses class labels to guide PCA, supervised (uses class information), maximizes variance while considering class structure, preserves class-relevant variance, computationally efficient, can improve class separation compared to unsupervised PCA.</p>
            </div>
            <div class="card">
                <h3>Watch-outs</h3>
                <p>Requires class labels (supervised), assumes linear relationships, may overfit to training classes, sensitive to class imbalance, limited by number of samples, may not capture non-linear class boundaries.</p>
            </div>
        </div>

        <div class="section">
            <h2>How It Works <span class="badge">Mathematical Foundation</span></h2>
            <div class="split">
                <div>
                    <p class="muted">Supervised PCA (SPCA) uses <span class="accent">class labels to guide PCA</span> — it computes a supervised covariance matrix that weights samples or features based on class information, then performs PCA on this modified covariance. Unlike standard PCA (which maximizes total variance), SPCA maximizes variance while considering class structure.</p>
                    <ul class="list">
                        <li><strong>Supervised Covariance</strong>: Compute covariance matrix using class-weighted samples or class-aware feature selection</li>
                        <li><strong>Class Weighting</strong>: Weight samples by class importance or balance classes in covariance computation</li>
                        <li><strong>Between-Class Variance</strong>: Emphasize variance that separates classes while maintaining total variance</li>
                        <li><strong>Eigenvalue Decomposition</strong>: Solve for principal components from supervised covariance matrix: Σ_supervised × w = λw</li>
                        <li><strong>Projection</strong>: Transform data: Y = X × W where W contains top k supervised principal components</li>
                        <li><strong>Component Limit</strong>: Maximum of min(n-1, p) components where n = samples, p = feature dimensions</li>
                    </ul>
                </div>
                <div class="callout">Tip: SPCA is supervised and requires class labels. It's ideal when you want PCA-like dimensionality reduction but with class guidance. For maximum class separation, use LDA instead.</div>
            </div>
        </div>

        <div class="section">
            <h2>Dimensionality Reduction Statistics <span class="badge">Interpretation</span></h2>
            <div class="kpi">
                <div class="item"><div class="label">Original Dimensions</div><div class="value">Number of input features</div></div>
                <div class="item"><div class="label">Reduced Dimensions</div><div class="value">Number of principal components</div></div>
                <div class="item"><div class="label">Classes</div><div class="value">Number of distinct classes</div></div>
                <div class="item"><div class="label">Variance Explained</div><div class="value">Cumulative variance ratio</div></div>
            </div>
        </div>

        <div class="section">
            <h2>Reading the Visualizations <span class="badge">Analysis</span></h2>
            <ul class="list">
                <li><strong>SPCA Projection</strong>: Shows data projected onto supervised principal components. Classes should be better separated than in standard PCA.</li>
                <li><strong>Variance Explained Plot</strong>: Shows cumulative variance explained by each component. First components capture most class-relevant variance.</li>
                <li><strong>Component Loadings</strong>: Shows how much each original feature contributes to each supervised principal component. High absolute values indicate features important for class-guided variance.</li>
                <li><strong>Eigenvalue Plot</strong>: Shows variance (eigenvalue) of each component. First component has highest supervised variance.</li>
                <li><strong>Class Separation</strong>: Compare class overlap in SPCA vs standard PCA. SPCA should show better class structure preservation.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Parameter Guidance <span class="badge">Best Practice</span></h2>
            <ul class="list">
                <li><strong>Number of Components</strong>: Maximum is min(n-1, p) where n = samples, p = feature dimensions. Use enough components to capture class-relevant variance (often 2-3 for visualization).</li>
                <li><strong>Standardization</strong>: SPCA requires standardization (mean=0, std=1) for features. Critical for meaningful variance calculations and fair feature weighting.</li>
                <li><strong>Class Labels</strong>: Required! SPCA is supervised and needs class information. Ensure labels are correct and all classes are represented.</li>
                <li><strong>Class Balance</strong>: Works with imbalanced classes, but balanced classes provide more stable supervised covariance estimates.</li>
                <li><strong>Supervision Strength</strong>: Balance between total variance (PCA-like) and class-guided variance. Higher supervision emphasizes class structure.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Data Preparation <span class="badge">Best Practice</span></h2>
            <ul class="list">
                <li><strong>Class Labels Required</strong>: SPCA is supervised and requires class labels for each sample. Labels must be categorical (numeric or string).</li>
                <li><strong>Standardize Features</strong>: Critical! Standardize features (mean=0, std=1) before SPCA. Essential for meaningful variance calculations.</li>
                <li><strong>Handle Missing Values</strong>: Remove or impute missing values before SPCA (SPCA cannot handle NaN).</li>
                <li><strong>Remove Outliers</strong>: Extreme outliers can distort covariance estimates, affecting supervised principal components.</li>
                <li><strong>Sample Size</strong>: Need sufficient samples (typically n > p) for stable covariance estimation. More samples per class = better results.</li>
                <li><strong>Class Representation</strong>: All classes should have at least a few samples. Classes with very few samples may not be well-represented in supervised covariance.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Common Pitfalls <span class="badge">Avoid</span></h2>
            <ul class="list">
                <li><strong>Missing Class Labels</strong>: SPCA requires class labels. Cannot be used for unsupervised dimensionality reduction.</li>
                <li><strong>Too Many Components</strong>: Requesting more than min(n-1, p) components will fail. Maximum components = minimum of (samples-1) and feature dimensions.</li>
                <li><strong>Not Standardizing</strong>: SPCA is sensitive to scaling. Always standardize features before applying SPCA.</li>
                <li><strong>Non-Linear Relationships</strong>: SPCA finds linear projections. Non-linear class boundaries may not be captured well.</li>
                <li><strong>Small Sample Size</strong>: With few samples, covariance matrices may be singular or unstable. Need n > p ideally.</li>
                <li><strong>Overfitting to Classes</strong>: Too much class guidance may overfit to training classes. Balance supervision strength.</li>
                <li><strong>Using SPCA for Maximum Separation</strong>: SPCA balances variance and class structure. For maximum class separation, use LDA instead.</li>
            </ul>
        </div>

        <div class="section">
            <h2>When to Use <span class="badge">Application</span></h2>
            <ul class="list">
                <li><strong>Class-Guided Visualization</strong>: When you want PCA-like visualization but with class structure preserved.</li>
                <li><strong>Feature Extraction for Classification</strong>: When you want to create features that capture class-relevant variance for downstream classification.</li>
                <li><strong>Class-Aware Dimensionality Reduction</strong>: When you want to reduce dimensions while preserving variance that separates classes.</li>
                <li><strong>Preprocessing for Classification</strong>: When you want to create supervised principal components that capture class structure before classification.</li>
                <li><strong>Balanced Variance and Class Structure</strong>: When you want to balance total variance (like PCA) with class-guided variance.</li>
                <li><strong>Supervised Dimensionality Reduction</strong>: When you have class labels and want to use them to guide PCA-like dimensionality reduction.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Alternatives to Consider <span class="badge">Comparison</span></h2>
            <ul class="list">
                <li><strong>vs LDA</strong>: SPCA balances variance and class structure; LDA maximizes class separation. Use SPCA for class-guided variance, LDA for maximum class separation.</li>
                <li><strong>vs PCA</strong>: SPCA is supervised (uses labels) and considers class structure; PCA is unsupervised (no labels) and maximizes total variance. Use SPCA for class-aware reduction, PCA for general dimensionality reduction.</li>
                <li><strong>vs CCA</strong>: SPCA uses class labels to guide PCA; CCA finds correlations between features and class indicators. Use SPCA for class-guided variance, CCA for correlation.</li>
                <li><strong>vs Regularized SPCA</strong>: Standard SPCA may fail with singular covariance matrices. Regularized SPCA adds regularization to handle this case.</li>
            </ul>
        </div>
    </div>
    <script src="../sidebar.js"></script>
</body>
</html>

