<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Learn about Correlation-based Feature Selection, a dimensionality reduction method. Comprehensive overview with theory, applications, and use cases for machine learning and data science.">
  <link rel="canonical" href="https://yourdomain.com/Dimensionality_Reduction/Correlation_Based_Overview.html">
  <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "name": "Correlation-based Feature Selection - Overview & Theory | Dimensionality Reduction | ML Tools",
  "description": "Learn about Correlation-based Feature Selection, a dimensionality reduction method. Comprehensive overview with theory, applications, and use cases for machine learning and data science.",
  "url": "https://yourdomain.com/Dimensionality_Reduction/Correlation_Based_Overview.html",
  "about": {
    "@type": "Thing",
    "name": "Correlation-based Feature Selection",
    "description": "Correlation-based Feature Selection algorithm for dimensionality reduction"
  },
  "headline": "Correlation-based Feature Selection",
  "articleSection": "Dimensionality Reduction"
}
  </script>
    <title>Correlation-based Feature Selection - Overview & Theory | Dimensionality Reduction | ML Tools</title>
    <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@200;300;400;600;700;800;900&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Nunito', sans-serif;
            color: #cfcfcf;
            background-color: #1e1e1e;
            margin: 0;
            padding: 0;
            display: flex;
            min-height: 100vh;
        }
        .sidebar {
            width: 250px;
            background-color: #171717;
            padding: 30px 20px;
            border-right: 1px solid #333;
            overflow-y: auto;
            height: 100vh;
        }
        .sidebar h1 { color: white; font-size: 1.8rem; font-weight: bold; margin-bottom: 30px; text-align: center; }
        .nav-section { margin-bottom: 30px; }
        .nav-section h3 { color: #dd8448; font-size: 0.9rem; font-weight: 600; text-transform: uppercase; margin-bottom: 15px; letter-spacing: 1px; }
        .nav-item { display: block; color: #676767; text-decoration: none; padding: 12px 15px; margin-bottom: 5px; border-radius: 8px; transition: all 0.3s ease; font-weight: 500; }
        .nav-item:hover { background-color: #2d2d2d; color: white; transform: translateX(5px); }
        .nav-item.active { background-color: #dd8448; color: white; }
        .nav-category { margin-bottom: 20px; }
        .nav-category-header { color: #dd8448; font-size: 0.85rem; font-weight: 600; text-transform: uppercase; padding: 10px 15px; cursor: pointer; border-radius: 8px; transition: background-color 0.3s ease; display: flex; justify-content: space-between; align-items: center; letter-spacing: 1px; }
        .nav-category-header:hover { background-color: #2d2d2d; }
        .nav-category-header .arrow { transition: transform 0.3s ease; font-size: 0.7rem; }
        .nav-category-header.collapsed .arrow { transform: rotate(-90deg); }
        .nav-subcategory { margin-left: 15px; margin-top: 10px; overflow: hidden; max-height: 1200px; transition: max-height 0.3s ease; }
        .nav-subcategory.collapsed { max-height: 0; }
        .nav-subheader { color: #999; font-size: 0.75rem; font-weight: 600; text-transform: uppercase; padding: 8px 15px; margin-top: 5px; letter-spacing: 0.5px; cursor: pointer; display: flex; justify-content: space-between; align-items: center; border-radius: 6px; transition: background-color 0.3s ease; }
        .nav-subheader:hover { background-color: #2d2d2d; }
        .nav-subheader .arrow { font-size: 0.6rem; transition: transform 0.3s ease; }
        .nav-subheader.collapsed .arrow { transform: rotate(-90deg); }
        .nav-subheader.category-header { color: #dd8448; font-size: 0.85rem; font-weight: 700; padding: 10px 15px; margin-top: 8px; letter-spacing: 1px; border: 1px solid rgba(221,132,72,0.3); }
        .nav-subheader.category-header:hover { background-color: rgba(221,132,72,0.15); border-color: rgba(221,132,72,0.5); }
        .nav-subgroup { overflow: hidden; max-height: 200px; transition: max-height 0.3s ease; padding-bottom: 5px; }
        .nav-subgroup.collapsed { max-height: 0; }
        .nav-subgroup.category-group { max-height: 1000px; }
        .nav-subgroup.category-group.collapsed { max-height: 0; }
        .nav-subheader.nested { margin-left: 10px; font-size: 0.7rem; }
        .nav-subgroup.nested { margin-left: 10px; max-height: 200px; }
        .nav-subheader.subcategory-header { color: #999; font-size: 0.75rem; font-weight: 600; padding: 8px 15px; margin-left: 10px; margin-top: 5px; letter-spacing: 0.5px; border: 1px solid rgba(153, 153, 153, 0.2); }
        .nav-subheader.subcategory-header:hover { background-color: rgba(45, 45, 45, 0.5); border-color: rgba(153, 153, 153, 0.4); }
        .nav-subgroup.subcategory-group { margin-left: 10px; max-height: 2000px; overflow: hidden; transition: max-height 0.3s ease; padding-bottom: 5px; }
        .nav-subgroup.subcategory-group.collapsed { max-height: 0; }
        .nav-item.sub { padding: 8px 15px; font-size: 0.9rem; margin-left: 10px; margin-bottom: 5px; }
        .main-content {
            flex: 1;
            padding: 40px;
        }
        .title {
            color: #ffffff;
            font-size: 2.2rem;
            font-weight: 800;
            margin: 0 0 8px 0;
        }
        .subtitle {
            color: #8a8a8a;
            font-size: 1rem;
            margin-bottom: 28px;
        }
        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
            gap: 20px;
            margin-bottom: 28px;
        }
        .card {
            background-color: #171717;
            border: 1px solid #333;
            border-radius: 12px;
            padding: 22px;
            transition: border-color 0.3s ease, transform 0.3s ease;
        }
        .card:hover { border-color: #dd8448; transform: translateY(-3px); }
        .card h3 { color: #ffffff; margin: 0 0 8px 0; font-size: 1.15rem; }
        .card p { color: #9a9a9a; margin: 0; line-height: 1.6; }
        .section {
            background-color: #171717;
            border: 1px solid #333;
            border-radius: 12px;
            padding: 24px;
            margin-bottom: 24px;
        }
        .section h2 {
            color: #ffffff;
            font-size: 1.5rem;
            margin: 0 0 12px 0;
        }
        .badge {
            display: inline-block;
            background: rgba(221,132,72,0.15);
            color: #ffb07a;
            border: 1px solid rgba(221,132,72,0.35);
            padding: 2px 8px;
            border-radius: 999px;
            font-size: 12px;
            margin-left: 8px;
        }
        .list { margin: 0; padding-left: 18px; color: #b0b0b0; }
        .list li { margin-bottom: 6px; }
        .split {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 16px;
        }
        .callout {
            background: #202020;
            border: 1px dashed #3a3a3a;
            border-radius: 10px;
            padding: 14px;
            color: #a9a9a9;
        }
        .kpi {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(160px, 1fr));
            gap: 10px;
        }
        .kpi .item {
            background: #1c1c1c;
            border: 1px solid #2a2a2a;
            border-radius: 10px;
            padding: 10px 12px;
        }
        .kpi .label { color: #8e8e8e; font-size: 12px; }
        .kpi .value { color: #ffffff; font-weight: 700; font-size: 18px; }
        .muted { color: #9a9a9a; }
        .accent { color: #dd8448; }
    </style>
</head>
<body>
    <div class="sidebar"></div>
    <div class="main-content">
        <h1 class="title">Correlation-based Feature Selection Overview <span class="badge">Guide</span></h1>
        <div class="subtitle">A filter method that removes highly correlated features — reduces redundancy and multicollinearity.</div>

        <div class="grid">
            <div class="card">
                <h3>Primary Uses</h3>
                <p>Removing redundant features, reducing multicollinearity, dimensionality reduction, feature selection, improving model interpretability, reducing overfitting risk.</p>
            </div>
            <div class="card">
                <h3>Strengths</h3>
                <p>Fast computation (O(p²)), simple to understand, removes redundant information, helps with multicollinearity, unsupervised (no labels needed), interpretable results, preserves feature meaning.</p>
            </div>
            <div class="card">
                <h3>Watch-outs</h3>
                <p>May remove important features if threshold too high, doesn't consider target variable, correlation doesn't imply causation, sensitive to outliers, linear relationships only (Pearson correlation).</p>
            </div>
        </div>

        <div class="section">
            <h2>How It Works <span class="badge">Mathematical Foundation</span></h2>
            <div class="split">
                <div>
                    <p class="muted">Correlation-based feature selection is a <span class="accent">filter method</span> that removes features based on their pairwise correlations. The assumption is that highly correlated features (|r| > threshold) provide redundant information, and one can be removed without significant information loss. It computes the correlation matrix between all feature pairs and removes one feature from each highly correlated pair.</p>
                    <ul class="list">
                        <li><strong>Correlation Calculation</strong>: For each pair of features (i, j), compute Pearson correlation rᵢⱼ = Σ(xᵢ - μᵢ)(xⱼ - μⱼ) / (n·σᵢ·σⱼ) where σ is standard deviation</li>
                        <li><strong>Correlation Matrix</strong>: Build a p×p symmetric matrix R where R[i][j] = correlation between feature i and feature j</li>
                        <li><strong>Threshold Comparison</strong>: Identify feature pairs where |rᵢⱼ| ≥ threshold (absolute correlation above threshold)</li>
                        <li><strong>Feature Removal</strong>: For each highly correlated pair, remove one feature (typically the one with higher average correlation to other features, or randomly)</li>
                        <li><strong>Iterative Process</strong>: Continue until no feature pairs exceed the threshold</li>
                        <li><strong>Absolute vs Signed</strong>: Use absolute correlation |r| to catch both positive and negative correlations, or use signed correlation to only remove positive correlations</li>
                    </ul>
                </div>
                <div class="callout">Tip: Correlation-based feature selection is most effective when features are standardized (mean=0, std=1). This ensures correlations are computed on a fair scale. Also, consider using absolute correlation to catch both positive and negative linear relationships. A common threshold is 0.7-0.9 for removing highly correlated features.</div>
            </div>
        </div>

        <div class="section">
            <h2>Feature Selection Statistics <span class="badge">Interpretation</span></h2>
            <div class="kpi">
                <div class="item"><div class="label">Original Features</div><div class="value">Number of input features</div></div>
                <div class="item"><div class="label">Selected Features</div><div class="value">Number of features kept (after removing highly correlated)</div></div>
                <div class="item"><div class="label">Removed Features</div><div class="value">Number of features removed (highly correlated with others)</div></div>
                <div class="item"><div class="label">Reduction Ratio</div><div class="value">Selected / Original features</div></div>
                <div class="item"><div class="label">Max Correlation</div><div class="value">Highest absolute correlation in remaining features</div></div>
                <div class="item"><div class="label">Avg Correlation</div><div class="value">Average absolute correlation in remaining features</div></div>
                <div class="item"><div class="label">Correlated Pairs</div><div class="value">Number of feature pairs above threshold</div></div>
            </div>
        </div>

        <div class="section">
            <h2>Reading the Visualizations <span class="badge">Analysis</span></h2>
            <ul class="list">
                <li><strong>Correlation Matrix Heatmap</strong>: Visual representation of all pairwise correlations. Red indicates high positive correlation, blue indicates high negative correlation. Diagonal is always 1.0 (self-correlation). Helps identify highly correlated feature pairs.</li>
                <li><strong>Correlation Distribution</strong>: Histogram showing distribution of all pairwise correlations. Helps identify natural breakpoints and appropriate threshold values. Most correlations should be near zero for independent features.</li>
                <li><strong>Highly Correlated Pairs</strong>: List or visualization of feature pairs with correlation above threshold. Shows which features are redundant and which one was removed from each pair.</li>
                <li><strong>Correlation vs Threshold</strong>: Shows how the number of selected features and correlated pairs changes with different threshold values. Helps choose optimal threshold.</li>
                <li><strong>Feature Network Graph</strong>: Network visualization showing features as nodes and correlations as edges. Highly correlated features appear as clusters. Useful for understanding feature relationships.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Parameter Guidance <span class="badge">Best Practice</span></h2>
            <ul class="list">
                <li><strong>Correlation Threshold</strong>: Common values: 0.7-0.9 (removes highly correlated features), 0.5-0.7 (more aggressive removal), 0.9-0.95 (conservative, removes only very highly correlated features). Lower threshold = more features removed.</li>
                <li><strong>Absolute vs Signed Correlation</strong>: Use absolute correlation (|r|) to catch both positive and negative correlations. Use signed correlation (r) if you only want to remove positive correlations (e.g., in some domain-specific cases).</li>
                <li><strong>Feature Removal Strategy</strong>: Remove the feature with higher average correlation to other features (most redundant), or remove randomly from each pair. The "highest average correlation" strategy is typically preferred.</li>
                <li><strong>Standardization</strong>: Always standardize features before computing correlations. This ensures correlations are computed on a fair scale and are not biased by feature scales.</li>
                <li><strong>Domain Knowledge</strong>: Consider domain-specific correlation expectations. Some features may naturally be highly correlated (e.g., height and weight) but both may be important. Use domain knowledge to guide threshold selection.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Data Preparation <span class="badge">Best Practice</span></h2>
            <ul class="list">
                <li><strong>Handle Missing Values</strong>: Remove or impute missing values before computing correlations. Correlation calculation requires complete data.</li>
                <li><strong>Standardize Features</strong>: Always standardize features (mean=0, std=1) before computing correlations. This ensures correlations are computed on a fair scale and are not biased by feature scales.</li>
                <li><strong>Numeric Features Only</strong>: Correlation-based selection works only on numeric features. Encode categorical variables appropriately first (e.g., one-hot encoding).</li>
                <li><strong>Outlier Handling</strong>: Pearson correlation is sensitive to outliers. Consider robust correlation measures (e.g., Spearman rank correlation) or remove outliers if needed.</li>
                <li><strong>Sample Size</strong>: Correlation estimates become more stable with larger sample sizes. For small datasets (n < 30), be cautious with threshold selection.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Common Pitfalls <span class="badge">Avoid</span></h2>
            <ul class="list">
                <li><strong>Not Standardizing Features</strong>: Computing correlations on unstandardized features can be misleading. Features with larger scales may appear more correlated. Always standardize first.</li>
                <li><strong>Too High Threshold</strong>: Setting threshold too high (e.g., > 0.95) may miss moderately correlated features that still provide redundant information. Consider threshold 0.7-0.9.</li>
                <li><strong>Too Low Threshold</strong>: Setting threshold too low (e.g., < 0.5) may remove too many features, including some that are only moderately correlated and still provide unique information.</li>
                <li><strong>Ignoring Feature Relationships</strong>: Correlation only captures linear relationships. Non-linear relationships may be missed. Consider using other methods (e.g., mutual information) for non-linear relationships.</li>
                <li><strong>Removing Important Features</strong>: Some highly correlated features may both be important (e.g., height and weight in health data). Use domain knowledge to guide feature removal.</li>
                <li><strong>Not Checking Removed Features</strong>: Always inspect which features were removed to ensure no important information was discarded.</li>
                <li><strong>Assuming Correlation = Causation</strong>: High correlation doesn't imply causation. Two features may be correlated due to a common cause, not because one causes the other.</li>
            </ul>
        </div>

        <div class="section">
            <h2>When to Use <span class="badge">Application</span></h2>
            <ul class="list">
                <li><strong>Multicollinearity Reduction</strong>: When you have features that are highly correlated and want to reduce multicollinearity (e.g., for linear models like regression).</li>
                <li><strong>Redundancy Removal</strong>: When you have redundant features (e.g., multiple measurements of the same underlying quantity) and want to keep only one.</li>
                <li><strong>Dimensionality Reduction</strong>: When you have many features (p > 50) and want to quickly remove redundant features to reduce dimensionality.</li>
                <li><strong>Preprocessing Step</strong>: As a preprocessing step before applying more sophisticated feature selection methods (wrapper or embedded methods).</li>
                <li><strong>Model Interpretability</strong>: When you want to improve model interpretability by removing redundant features that don't add unique information.</li>
                <li><strong>Overfitting Prevention</strong>: When you want to reduce the risk of overfitting by removing redundant features that may cause models to focus on noise.</li>
                <li><strong>Fast Feature Selection</strong>: When you need a fast feature selection method for large datasets (O(p²) complexity).</li>
            </ul>
        </div>

        <div class="section">
            <h2>Alternatives to Consider <span class="badge">Comparison</span></h2>
            <ul class="list">
                <li><strong>vs Variance Threshold</strong>: Correlation-based removes redundant features (high correlation). Variance Threshold removes low-variance features. Use both: first remove low variance, then remove high correlation.</li>
                <li><strong>vs Wrapper Methods (e.g., RFE)</strong>: Correlation-based is fast but doesn't consider feature interactions or target variable. Wrapper methods are slower but more accurate. Use correlation-based for preprocessing, wrapper methods for final selection.</li>
                <li><strong>vs Embedded Methods (e.g., Lasso)</strong>: Correlation-based is unsupervised and simple. Embedded methods consider target variable and feature interactions but require model training. Use correlation-based for quick filtering, embedded methods for model-aware selection.</li>
                <li><strong>vs Mutual Information</strong>: Correlation captures only linear relationships. Mutual information captures both linear and non-linear relationships. Use correlation for linear relationships, mutual information for non-linear.</li>
                <li><strong>vs PCA</strong>: Correlation-based removes features. PCA creates new features (components). Use correlation-based to reduce feature count while preserving original features, PCA to reduce dimensionality while preserving variance.</li>
            </ul>
        </div>
    </div>
    <script src="../sidebar.js"></script>
</body>
</html>

