<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Learn about Bayesian Network, a anomaly detection method. Comprehensive overview with theory, applications, and use cases for machine learning and data science.">
  <link rel="canonical" href="https://yourdomain.com/Anomaly_Detection/Bayesian_Network_Overview.html">
  <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "name": "Bayesian Network - Overview & Theory | Anomaly Detection | ML Tools",
  "description": "Learn about Bayesian Network, a anomaly detection method. Comprehensive overview with theory, applications, and use cases for machine learning and data science.",
  "url": "https://yourdomain.com/Anomaly_Detection/Bayesian_Network_Overview.html",
  "about": {
    "@type": "Thing",
    "name": "Bayesian Network",
    "description": "Bayesian Network algorithm for anomaly detection"
  },
  "headline": "Bayesian Network",
  "articleSection": "Anomaly Detection"
}
  </script>
    <title>Bayesian Network - Overview & Theory | Anomaly Detection | ML Tools</title>
    <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@200;300;400;600;700;800;900&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Nunito', sans-serif;
            color: #cfcfcf;
            background-color: #1e1e1e;
            margin: 0;
            padding: 0;
            display: flex;
            min-height: 100vh;
        }
        .sidebar {
            width: 250px;
            background-color: #171717;
            padding: 30px 20px;
            border-right: 1px solid #333;
            overflow-y: auto;
            height: 100vh;
        }
        .sidebar h1 { color: white; font-size: 1.8rem; font-weight: bold; margin-bottom: 30px; text-align: center; }
        .nav-section { margin-bottom: 30px; }
        .nav-section h3 { color: #dd8448; font-size: 0.9rem; font-weight: 600; text-transform: uppercase; margin-bottom: 15px; letter-spacing: 1px; }
        .nav-item { display: block; color: #676767; text-decoration: none; padding: 12px 15px; margin-bottom: 5px; border-radius: 8px; transition: all 0.3s ease; font-weight: 500; }
        .nav-item:hover { background-color: #2d2d2d; color: white; transform: translateX(5px); }
        .nav-item.active { background-color: #dd8448; color: white; }
        .nav-category { margin-bottom: 20px; }
        .nav-category-header { color: #dd8448; font-size: 0.85rem; font-weight: 600; text-transform: uppercase; padding: 10px 15px; cursor: pointer; border-radius: 8px; transition: background-color 0.3s ease; display: flex; justify-content: space-between; align-items: center; letter-spacing: 1px; }
        .nav-category-header:hover { background-color: #2d2d2d; }
        .nav-category-header .arrow { transition: transform 0.3s ease; font-size: 0.7rem; }
        .nav-category-header.collapsed .arrow { transform: rotate(-90deg); }
        .nav-subcategory { margin-left: 15px; margin-top: 10px; overflow: hidden; max-height: 1200px; transition: max-height 0.3s ease; }
        .nav-subcategory.collapsed { max-height: 0; }
        .nav-subheader { color: #999; font-size: 0.75rem; font-weight: 600; text-transform: uppercase; padding: 8px 15px; margin-top: 5px; letter-spacing: 0.5px; cursor: pointer; display: flex; justify-content: space-between; align-items: center; border-radius: 6px; transition: background-color 0.3s ease; }
        .nav-subheader:hover { background-color: #2d2d2d; }
        .nav-subheader .arrow { font-size: 0.6rem; transition: transform 0.3s ease; }
        .nav-subheader.collapsed .arrow { transform: rotate(-90deg); }
        .nav-subheader.category-header { color: #dd8448; font-size: 0.85rem; font-weight: 700; padding: 10px 15px; margin-top: 8px; letter-spacing: 1px; border: 1px solid rgba(221,132,72,0.3); }
        .nav-subheader.category-header:hover { background-color: rgba(221,132,72,0.15); border-color: rgba(221,132,72,0.5); }
        .nav-subgroup { overflow: hidden; max-height: 200px; transition: max-height 0.3s ease; padding-bottom: 5px; }
        .nav-subgroup.collapsed { max-height: 0; }
        .nav-subgroup.category-group { max-height: 1000px; }
        .nav-subgroup.category-group.collapsed { max-height: 0; }
        .nav-subheader.nested { margin-left: 10px; font-size: 0.7rem; }
        .nav-subgroup.nested { margin-left: 10px; max-height: 200px; }
        .nav-subheader.subcategory-header { color: #999; font-size: 0.75rem; font-weight: 600; padding: 8px 15px; margin-left: 10px; margin-top: 5px; letter-spacing: 0.5px; border: 1px solid rgba(153, 153, 153, 0.2); }
        .nav-subheader.subcategory-header:hover { background-color: rgba(45, 45, 45, 0.5); border-color: rgba(153, 153, 153, 0.4); }
        .nav-subgroup.subcategory-group { margin-left: 10px; max-height: 800px; overflow: hidden; transition: max-height 0.3s ease; padding-bottom: 5px; }
        .nav-subgroup.subcategory-group.collapsed { max-height: 0; }
        .nav-item.sub { padding: 8px 15px; font-size: 0.9rem; margin-left: 10px; margin-bottom: 5px; }
        .main-content {
            flex: 1;
            padding: 40px;
        }
        .title {
            color: #ffffff;
            font-size: 2.2rem;
            font-weight: 800;
            margin: 0 0 8px 0;
        }
        .subtitle {
            color: #8a8a8a;
            font-size: 1rem;
            margin-bottom: 28px;
        }
        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
            gap: 20px;
            margin-bottom: 28px;
        }
        .card {
            background-color: #171717;
            border: 1px solid #333;
            border-radius: 12px;
            padding: 22px;
            transition: border-color 0.3s ease, transform 0.3s ease;
        }
        .card:hover { border-color: #dd8448; transform: translateY(-3px); }
        .card h3 { color: #ffffff; margin: 0 0 8px 0; font-size: 1.15rem; }
        .card p { color: #9a9a9a; margin: 0; line-height: 1.6; }
        .section {
            background-color: #171717;
            border: 1px solid #333;
            border-radius: 12px;
            padding: 24px;
            margin-bottom: 24px;
        }
        .section h2 {
            color: #ffffff;
            font-size: 1.5rem;
            margin: 0 0 12px 0;
        }
        .badge {
            display: inline-block;
            background: rgba(221,132,72,0.15);
            color: #ffb07a;
            border: 1px solid rgba(221,132,72,0.35);
            padding: 2px 8px;
            border-radius: 999px;
            font-size: 12px;
            margin-left: 8px;
        }
        .list { margin: 0; padding-left: 18px; color: #b0b0b0; }
        .list li { margin-bottom: 6px; }
        .split {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 16px;
        }
        .callout {
            background: #202020;
            border: 1px dashed #3a3a3a;
            border-radius: 10px;
            padding: 14px;
            color: #a9a9a9;
        }
        .kpi {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(160px, 1fr));
            gap: 10px;
        }
        .kpi .item {
            background: #1c1c1c;
            border: 1px solid #2a2a2a;
            border-radius: 10px;
            padding: 10px 12px;
        }
        .kpi .label { color: #8e8e8e; font-size: 12px; }
        .kpi .value { color: #ffffff; font-weight: 700; font-size: 18px; }
        .muted { color: #9a9a9a; }
        .accent { color: #dd8448; }
        .legend {
            display: flex;
            gap: 12px;
            flex-wrap: wrap;
        }
        .legend .chip {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            background: #202020;
            border: 1px solid #333;
            border-radius: 999px;
            padding: 6px 10px;
            color: #cfcfcf;
            font-size: 12px;
        }
        .chip .dot { width: 8px; height: 8px; border-radius: 100%; background: #dd8448; display: inline-block; }
    </style>
</head>
<body>
    <div class="sidebar"></div>
    <div class="main-content">
        <h1 class="title">Bayesian Network Overview <span class="badge">Guide</span></h1>
        <div class="subtitle">Probabilistic model-based anomaly detection: learn conditional dependencies between variables to model normal data — points with low joint probability (high negative log-likelihood) are anomalies.</div>

        <div class="grid">
            <div class="card">
                <h3>Primary Uses</h3>
                <p>Anomaly detection, dependency modeling, causal inference, probabilistic reasoning, detecting outliers in structured data, modeling relationships between variables, handling missing data, uncertainty quantification.</p>
            </div>
            <div class="card">
                <h3>Strengths</h3>
                <p>Models conditional dependencies explicitly, provides interpretable structure (DAG), handles missing data naturally, captures variable relationships, provides probabilistic scores, can incorporate domain knowledge, efficient inference for structured models.</p>
            </div>
            <div class="card">
                <h3>Watch-outs</h3>
                <p>Requires learning or specifying network structure, structure learning is NP-hard, sensitive to structure quality, can be computationally expensive for large networks, requires sufficient data to estimate conditional probabilities, discrete vs continuous variable handling, may miss complex non-linear dependencies.</p>
            </div>
        </div>

        <div class="section">
            <h2>How It Works <span class="badge">Mathematical Foundation</span></h2>
            <div class="split">
                <div>
                    <p class="muted">A <span class="accent">Bayesian Network</span> is a directed acyclic graph (DAG) that represents conditional dependencies between variables. Each node represents a variable, and edges represent conditional dependencies. The network encodes a joint probability distribution using the chain rule: P(X₁, X₂, ..., Xₙ) = Πᵢ P(Xᵢ | parents(Xᵢ)). For anomaly detection, the network is learned from normal data, and points with low joint probability (high negative log-likelihood) are flagged as anomalies.</p>
                    <ul class="list">
                        <li><strong>Network Structure</strong>: Directed acyclic graph (DAG) representing variable dependencies</li>
                        <li><strong>Conditional Probability Tables (CPTs)</strong>: P(Xᵢ | parents(Xᵢ)) learned from data for each variable</li>
                        <li><strong>Joint Probability</strong>: P(X₁, ..., Xₙ) = Πᵢ P(Xᵢ | parents(Xᵢ)) using chain rule</li>
                        <li><strong>Structure Learning</strong>: Learn DAG from data (e.g., score-based, constraint-based, or hybrid methods)</li>
                        <li><strong>Parameter Learning</strong>: Estimate CPTs given structure (maximum likelihood or Bayesian estimation)</li>
                        <li><strong>Anomaly Score</strong>: negative log-likelihood -log(P(x)); higher = more anomalous</li>
                    </ul>
                </div>
                <div class="callout">Tip: Bayesian Networks learn the conditional dependencies between variables from normal data. Anomalies violate these learned dependencies, resulting in low joint probability and high negative log-likelihood. The network structure captures how variables relate to each other.</div>
            </div>
        </div>

        <div class="section">
            <h2>Anomaly Detection Statistics <span class="badge">Interpretation</span></h2>
            <div class="kpi">
                <div class="item"><div class="label">Negative Log-Likelihood</div><div class="value">-log(P(x)) where P(x) is joint probability under network (higher = more anomalous)</div></div>
                <div class="item"><div class="label">Network Structure</div><div class="value">Learned DAG representing variable dependencies</div></div>
                <div class="item"><div class="label">Number of Edges</div><div class="value">Number of conditional dependencies in network</div></div>
                <div class="item"><div class="label">Anomaly Count</div><div class="value">Number of points exceeding threshold</div></div>
                <div class="item"><div class="label">Anomaly Rate</div><div class="value">Percentage of data flagged as anomalies</div></div>
            </div>
            <p class="muted" style="margin-top: 12px;">Points with negative log-likelihood > threshold are flagged as anomalies. Bayesian Networks learn the joint probability distribution of normal data by modeling conditional dependencies. Anomalies violate these learned dependencies, resulting in low joint probability and high negative log-likelihood scores.</p>
        </div>

        <div class="section">
            <h2>Reading the Graphs <span class="badge">2D</span></h2>
            <ul class="list">
                <li><strong>Data Visualization</strong>: points colored by negative log-likelihood; red = anomalies (above threshold), blue/green = normal.</li>
                <li><strong>Probability Heatmap</strong>: color-coded joint probability across the space; darker colors = lower probability (more anomalous).</li>
                <li><strong>Likelihood Distribution</strong>: histogram of negative log-likelihoods; helps identify natural threshold cutoff (typically right tail contains outliers).</li>
                <li><strong>Threshold Line</strong>: threshold visualized in distribution; adjust to change sensitivity.</li>
                <li><strong>Network Visualization</strong>: shows learned DAG structure and conditional dependencies; helps understand variable relationships.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Reading the Graphs <span class="badge">3D</span></h2>
            <ul class="list">
                <li><strong>3D Scatter Plot</strong>: points colored by negative log-likelihood; rotate to inspect anomalies in 3D space.</li>
                <li><strong>Likelihood Distribution</strong>: histogram helps set threshold; 3D typically shows different likelihood patterns.</li>
                <li><strong>Anomaly Points</strong>: clearly marked in red; inspect from different angles to understand probability distribution.</li>
                <li><strong>Network Structure</strong>: 3D network visualization showing dependencies between X, Y, Z variables.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Parameter Guidance <span class="badge">Best Practice</span></h2>
            <ul class="list">
                <li><strong>Structure Learning Method</strong>: Score-based (e.g., BIC, AIC) vs constraint-based (e.g., PC algorithm) vs hybrid. Score-based is common for small networks. Start with score-based using BIC.</li>
                <li><strong>Discretization</strong>: For continuous variables, discretize into bins. Too few bins = loses information, too many = sparse CPTs. Typically 3-5 bins per variable.</li>
                <li><strong>Maximum Parents</strong>: Limit number of parents per node to control complexity. Too many = overfitting, too few = underfitting. Typically 1-3 for 2D/3D data.</li>
                <li><strong>Prior Knowledge</strong>: Incorporate domain knowledge about variable dependencies if available. Can constrain structure learning.</li>
                <li><strong>Smoothing</strong>: Use Laplace smoothing (pseudo-counts) for CPT estimation to handle sparse data. Typical: α = 0.1 to 1.0.</li>
                <li><strong>Threshold</strong>: Set based on negative log-likelihood distribution. Use percentile-based (e.g., top 5% or 1% as anomalies) or absolute threshold based on domain knowledge.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Data Preparation <span class="badge">Best Practice</span></h2>
            <ul class="list">
                <li>Normalize or standardize features before discretization (if using continuous variables).</li>
                <li>Discretize continuous variables into meaningful bins (equal-width or equal-frequency).</li>
                <li>Handle missing values before training (impute or use network's ability to handle missing data).</li>
                <li>Ensure sufficient training data; Bayesian Networks need enough data to estimate CPTs reliably (typically n > 10 * states per variable).</li>
                <li>Remove or mark outliers in training data if you want to learn only normal patterns (critical for anomaly detection).</li>
                <li>For high-dimensional data, consider feature selection or limit network complexity (max parents).</li>
                <li>Check for conditional dependencies in normal data to guide structure learning.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Common Pitfalls <span class="badge">Avoid</span></h2>
            <ul class="list">
                <li>Poor network structure → misses important dependencies or includes spurious ones, poor model fit.</li>
                <li>Overfitting structure → too many edges, learns noise, poor generalization.</li>
                <li>Underfitting structure → too few edges, misses dependencies, poor model fit.</li>
                <li>Not discretizing properly → continuous variables need discretization, wrong bins = information loss.</li>
                <li>Including anomalies in training → learns to model anomalies as normal, defeats purpose.</li>
                <li>Sparse CPTs → insufficient data for some parent configurations, unreliable probabilities.</li>
                <li>Too many parents → exponential growth in CPT size, requires more data, prone to overfitting.</li>
                <li>Wrong threshold → too high = misses anomalies, too low = too many false positives.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Bayesian Network vs Other Methods <span class="badge">Comparison</span></h2>
            <ul class="list">
                <li><strong>vs GMM</strong>: Bayesian Networks model conditional dependencies explicitly, but more complex structure learning. GMM assumes independence or simple correlations.</li>
                <li><strong>vs Single Gaussian (Mahalanobis)</strong>: Bayesian Networks can model complex dependencies, but requires structure learning. Single Gaussian assumes simple covariance structure.</li>
                <li><strong>vs Density-based (LOF, LOCI)</strong>: Bayesian Networks learn parametric model with structure, but assumes discrete or discretized variables. Density-based methods are non-parametric but slower.</li>
                <li><strong>vs Neural Networks (Autoencoder, VAE)</strong>: Bayesian Networks are interpretable and handle missing data well, but less flexible for non-linear patterns. Neural networks can learn complex patterns but are black boxes.</li>
                <li><strong>Structure Advantage</strong>: Provides interpretable dependency structure, enabling understanding of variable relationships.</li>
                <li><strong>Missing Data</strong>: Naturally handles missing data through probabilistic inference.</li>
            </ul>
        </div>

        <div class="section">
            <h2>When to Use <span class="badge">Application</span></h2>
            <ul class="list">
                <li><strong>Structured Data</strong>: when variables have known or learnable conditional dependencies.</li>
                <li><strong>Interpretable Models</strong>: when you need to understand variable relationships (network structure).</li>
                <li><strong>Missing Data</strong>: when data has missing values (networks handle this naturally).</li>
                <li><strong>Domain Knowledge</strong>: when you have prior knowledge about variable dependencies to incorporate.</li>
                <li><strong>Moderate Dimensionality</strong>: works well for 2D-10D data. Higher dimensions require structure constraints.</li>
                <li><strong>Discrete or Discretizable Data</strong>: when variables are discrete or can be meaningfully discretized.</li>
                <li><strong>Probabilistic Reasoning</strong>: when you need probabilistic inference and uncertainty quantification.</li>
                <li><strong>Causal Modeling</strong>: when you want to model causal relationships between variables.</li>
            </ul>
        </div>
    </div>
    <script src="../sidebar.js"></script>
</body>
</html>

